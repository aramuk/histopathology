{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Veggie16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TV-RGMssmSj"
      },
      "source": [
        "# Veggie16\n",
        "\n",
        "An exploration of how to adapt the VGG-16 CNN architecture, and modify the structure of the classification layer. Explores how to transfer learn and freeze the weights for the convolutional layers.\n",
        "\n",
        "The developed Veggie16 is evaluated on the MNIST dataset.\n",
        "\n",
        "This notebook is primarily inspired by the provided [CS284A CNN example](https://github.com/xhxuciedu/CS284A/blob/master/convolutional_neural_net.ipynb)\n",
        "\n",
        "Make sure the runtime type of this notebook has a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgIQqZk8V52P"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import vgg16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omtgpfK5Iu_H"
      },
      "source": [
        "### Use GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_CPNvBZIW-P"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HngJS7KusUNo"
      },
      "source": [
        "### Define training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AabkdzDPd2Ce"
      },
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 25\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPXHoJrdseAy"
      },
      "source": [
        "### Define a CNN based on VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj_z32O0QTWU"
      },
      "source": [
        "class Veggie16(nn.Module):\n",
        "    \"\"\"A model that adapts the VGG-16 architecture.\n",
        "\n",
        "    This network applies transfer learning to learn the parameters\n",
        "    of VGG-16, and freezes those layers of the model. The classification\n",
        "    layer of the architecture is modified and will be retrained to \n",
        "    predict the desired number of output classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        \"\"\"Creates a Veggie16 network.\n",
        "\n",
        "        Params:\n",
        "            num_classes - The number of output classes to predict\n",
        "        \"\"\"\n",
        "        super(Veggie16, self).__init__()\n",
        "        architecture = vgg16(pretrained=True)\n",
        "        self.features = architecture.features\n",
        "        for param in self.features:\n",
        "            param.requires_grad = False\n",
        "        self.avgpool = architecture.avgpool\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Does a forward pass on an image x.\"\"\"\n",
        "        x_stack = torch.cat((x,x,x), 1) # MNIST is grayscale, but ImageNet is RGB\n",
        "        out = self.features(x_stack)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "model = Veggie16(10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENqLp2QhUf1x"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOhfU1lyotEB"
      },
      "source": [
        "### Load MNIST Data Set for Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDmogycUnds5"
      },
      "source": [
        "data_dir = '../data'\n",
        "train_dataset = torchvision.datasets.MNIST(root=data_dir, \n",
        "                                           train=True, \n",
        "                                           transform=transforms.Compose([\n",
        "                                               transforms.Resize(256),\n",
        "                                               transforms.CenterCrop(224),\n",
        "                                               transforms.ToTensor()\n",
        "                                           ]),\n",
        "                                           download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=data_dir,\n",
        "                                          train=False,\n",
        "                                          transform=transforms.Compose([\n",
        "                                               transforms.Pad(114),\n",
        "                                               transforms.ToTensor()\n",
        "                                           ]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXT-ETCkoyIb"
      },
      "source": [
        "### Initialize `Dataloader` instances for reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDcuU_1-o36J"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drXyYzmJo-sq"
      },
      "source": [
        "### Train `Veggie16` model on MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re2yPTMEo7iF"
      },
      "source": [
        "num_steps = len(train_loader)\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    for i, (images, labels) in enumerate(train_loader, start=1):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch [{epoch}/{num_epochs}], Step [{i}/{num_steps}], Loss: {loss.item():.6f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axKl0OvRqwUS"
      },
      "source": [
        "### Evaluate the effectiveness of `Veggie16`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GqyY31Dqt55"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        total += labels.size(0)\n",
        "        correct += (outputs == labels).sum().item()\n",
        "    print(f'Test Accuracy of Veggie16: {(100*(correct/total)):.6f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}